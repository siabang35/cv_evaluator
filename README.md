# CV Evaluator - Full Stack Application

AI-powered CV and project report evaluation system with **Next.js frontend** and **FastAPI backend**.

---

## ğŸ“‚ Project Structure

```
cv-evaluator/
â”œâ”€â”€ backend/                 # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ tasks/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ app/                     # Next.js frontend
â”œâ”€â”€ components/
â””â”€â”€ scripts/                 # Database scripts
```

---

## âœ¨ Features

- ğŸ“„ **Document Upload**: Upload CV and project reports (PDF)
- ğŸ¤– **AI Evaluation**: LLM-powered evaluation with RAG context retrieval using Groq
- ğŸ”’ **Local Embeddings**: Privacy-focused local embeddings with Sentence Transformers
- âš¡ **Async Processing**: Celery-based background job processing
- â± **Real-time Status**: Poll for evaluation status and results
- ğŸ“ **Detailed Feedback**: Comprehensive scoring and feedback
- ğŸ’° **Cost-Effective**: Uses Groq's free tier for fast LLM inference

---

## ğŸ›  Tech Stack

### Frontend
- Next.js 15 (App Router)
- TypeScript
- Tailwind CSS v4
- shadcn/ui components

### Backend
- FastAPI
- PostgreSQL (Supabase) with pgvector
- ChromaDB (Vector database)
- **Groq API** (llama-3.3-70b-versatile) - Fast & free LLM inference
- **Sentence Transformers** (all-MiniLM-L6-v2) - Local embeddings
- Celery + Redis
- Python 3.11+

---

## ğŸš€ Setup Instructions

### Prerequisites

- Node.js 18+
- Python 3.11+
- PostgreSQL with pgvector
- Redis
- OpenAI API key

### Backend Setup

```bash
# 1. Navigate to backend directory
cd backend

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure environment variables
cp .env.example .env
# Edit .env with your credentials

# 5. Run database migrations
# Execute SQL scripts in scripts/ folder

# 6. Start Redis
redis-server

# 7. Start Celery worker
celery -A app.tasks.evaluation_tasks worker --loglevel=info

# 8. Start FastAPI server
uvicorn app.main:app --reload
```

### Frontend Setup

```bash
# 1. Install dependencies
npm install

# 2. Configure environment variables
cp .env.local.example .env.local
# Edit .env.local with API URL

# 3. Start development server
npm run dev

# 4. Open in browser
http://localhost:3000
```

---

## ğŸ“¡ API Endpoints

- `POST /api/upload` â†’ Upload CV and project report
- `POST /api/evaluate` â†’ Trigger evaluation pipeline
- `GET /api/result/{id}` â†’ Get evaluation results

---

## âš™ï¸ Environment Variables

### Backend (`.env`)
```ini
# Database
DATABASE_URL=postgresql://...
SUPABASE_URL=https://...
SUPABASE_KEY=...
SUPABASE_SERVICE_KEY=...

# Groq API (free tier available)
GROQ_API_KEY=gsk_...
GROQ_MODEL=llama-3.3-70b-versatile

# Embeddings (local, no API key needed)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Redis
REDIS_URL=redis://localhost:6379/0

# Application
SECRET_KEY=your-secret-key
APP_ENV=development
```

### Frontend (`.env.local`)
```ini
NEXT_PUBLIC_API_URL=http://localhost:8000/api
NEXT_PUBLIC_MAX_FILE_SIZE=10485760
NEXT_PUBLIC_POLL_INTERVAL=3000
```

---

## ğŸ’¡ Why Groq + Sentence Transformers?

### ğŸš€ Groq Advantages
- **10-100x faster** than traditional LLM providers
- **Free tier** with generous rate limits
- **High quality** models (Llama 3.3, Mixtral, etc.)
- **Low latency** for real-time applications

### ğŸ”’ Sentence Transformers Advantages
- **100% free** - no API costs
- **Privacy-focused** - embeddings generated locally
- **Fast** - no network latency
- **High quality** - state-of-the-art embedding models
- **Offline capable** - works without internet after initial download

---

## ğŸ“Š Performance Benchmarks

- **LLM Inference**: ~500-1000 tokens/sec with Groq (vs ~50-100 with OpenAI)
- **Embeddings**: ~1000 documents/sec locally (vs ~10-50 with API calls)
- **Total Evaluation Time**: ~30-60 seconds for full CV + project analysis

---

## ğŸ’µ Cost Comparison

| Component       | This Setup   | Traditional Setup      |
|-----------------|--------------|------------------------|
| LLM Inference   | Free (Groq)  | $0.01-0.03/1K tokens   |
| Embeddings      | Free (Local) | $0.0001/1K tokens      |
| Vector DB       | Free (ChromaDB) | $0-50/month         |
| **Total/month** | **$0**       | **$50-200**            |

---

## ğŸ“œ License

MIT
